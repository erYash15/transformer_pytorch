{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Model Training Notebook\n\nThis notebook demonstrates how to train a transformer model in the Kaggle environment. We will go through the steps of setting up the environment, loading data, preprocessing, training the model, and evaluating its performance.\n\nHowever, Architecture is available in the github repo: **eryash15/transformer_pytorch**\n\n## Kaggle\nKaggle provides the perfect environment for training the model with\n\n## Setup\n\nFirst, let's load the code repository and import the necessary libraries.","metadata":{}},{"cell_type":"markdown","source":"## Clone the tranformer repositry (it doesn't require any external data)","metadata":{}},{"cell_type":"code","source":"! git clone https://github.com/erYash15/transformer_pytorch.git","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:21.122805Z","iopub.execute_input":"2024-06-18T13:18:21.123161Z","iopub.status.idle":"2024-06-18T13:18:23.557648Z","shell.execute_reply.started":"2024-06-18T13:18:21.123132Z","shell.execute_reply":"2024-06-18T13:18:23.556012Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'transformer_pytorch'...\nremote: Enumerating objects: 53, done.\u001b[K\nremote: Counting objects: 100% (53/53), done.\u001b[K\nremote: Compressing objects: 100% (43/43), done.\u001b[K\nremote: Total 53 (delta 6), reused 35 (delta 5), pack-reused 0\u001b[K\nUnpacking objects: 100% (53/53), 2.81 MiB | 6.86 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nfrom pathlib import Path\n\nimport torchmetrics\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:23.559532Z","iopub.execute_input":"2024-06-18T13:18:23.559978Z","iopub.status.idle":"2024-06-18T13:18:41.906418Z","shell.execute_reply.started":"2024-06-18T13:18:23.559932Z","shell.execute_reply":"2024-06-18T13:18:41.905611Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-18 13:18:32.802206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-18 13:18:32.802333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-18 13:18:32.933112: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append('../scripts')\nsys.path.append('/kaggle/working/transformer_pytorch/scripts')\n\n\nfrom model import build_transformer\nfrom utils import count_parameters\nfrom config import get_config, get_weights_file_path, latest_weights_file_path","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.907662Z","iopub.execute_input":"2024-06-18T13:18:41.908222Z","iopub.status.idle":"2024-06-18T13:18:41.919970Z","shell.execute_reply.started":"2024-06-18T13:18:41.908195Z","shell.execute_reply":"2024-06-18T13:18:41.919083Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    \"\"\"Generates the sentence\"\"\"\n    for item in ds:\n        yield item['translation'][lang]","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.922215Z","iopub.execute_input":"2024-06-18T13:18:41.922488Z","iopub.status.idle":"2024-06-18T13:18:41.937149Z","shell.execute_reply.started":"2024-06-18T13:18:41.922465Z","shell.execute_reply":"2024-06-18T13:18:41.936195Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Example\ndef test_get_all_sentences():\n    input_dataset = [\n        {'id': 1, 'translation': {'en': 'Hello', 'fr': 'Bonjour'}},\n        {'id': 2, 'translation': {'en': 'Goodbye', 'fr': 'Au revoir'}}\n    ]    \n    expected_output = ['Bonjour', 'Au revoir'] \n\n    result_output = []\n\n    lang = 'fr'\n    for sentence in get_all_sentences(input_dataset, lang):\n        result_output.append(sentence)\n    \n    assert expected_output == result_output \n    print(\"test_get_all_sentences: test success\")\n    \ntest_get_all_sentences()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.938398Z","iopub.execute_input":"2024-06-18T13:18:41.938746Z","iopub.status.idle":"2024-06-18T13:18:41.947692Z","shell.execute_reply.started":"2024-06-18T13:18:41.938713Z","shell.execute_reply":"2024-06-18T13:18:41.946821Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"test_get_all_sentences: test success\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        print(f\"{tokenizer_path} does not exists.\")\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        print(f\"{tokenizer_path} exists.\")\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.948825Z","iopub.execute_input":"2024-06-18T13:18:41.949091Z","iopub.status.idle":"2024-06-18T13:18:41.958426Z","shell.execute_reply.started":"2024-06-18T13:18:41.949068Z","shell.execute_reply":"2024-06-18T13:18:41.957704Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Example\ndef test_get_or_build_tokenizer():\n\n    config = {\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    dataset = [\n        {'id': 1, 'translation': {'en': 'Hello world', 'fr': 'Bonjour tout le monde'}},\n        {'id': 2, 'translation': {'en': 'How are you?', 'fr': 'Comment ça va ?'}}\n    ]\n\n    lang = 'fr'\n    tokenizer_not_exist = get_or_build_tokenizer(config, dataset, lang)\n    tokenizer = get_or_build_tokenizer(config, dataset, lang)\n    \n    assert tokenizer.encode(\"Bonjour tout le monde yash\").tokens == ['Bonjour', 'tout', 'le', 'monde', '[UNK]']\n    assert tokenizer.encode(\"Bonjour tout le monde yash\").ids == [5, 9, 7, 8, 0]\n    \n    print(\"test_get_or_build_tokenizer: test success\")\n    \ntest_get_or_build_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.959584Z","iopub.execute_input":"2024-06-18T13:18:41.959911Z","iopub.status.idle":"2024-06-18T13:18:41.992000Z","shell.execute_reply.started":"2024-06-18T13:18:41.959888Z","shell.execute_reply":"2024-06-18T13:18:41.991061Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tokenizer_fr.json does not exists.\ntokenizer_fr.json exists.\ntest_get_or_build_tokenizer: test success\n","output_type":"stream"}]},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n\n        # since all these token for both lang remain same we can use tokenizer_tgt or tokenizer_src\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    @staticmethod\n    def causal_mask(size):\n        mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n        return mask == 0\n\n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, index):\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # SOS and EOS\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # no EOS or SOS\n\n        if enc_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n        \n        # add SOS and EOS to the source text\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        return {\n            \"encoder_input\": encoder_input, #seq_len\n            \"decoder_input\": decoder_input, #seq_len\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & BilingualDataset.causal_mask(decoder_input.size(0)), # (1,seq_len) & (1, seq_len, seq_len)\n            \"label\": label,\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text\n        }","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:41.993510Z","iopub.execute_input":"2024-06-18T13:18:41.993994Z","iopub.status.idle":"2024-06-18T13:18:42.009419Z","shell.execute_reply.started":"2024-06-18T13:18:41.993961Z","shell.execute_reply":"2024-06-18T13:18:42.008500Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Example \ndef test_BilingualDataset():\n    dataset = [\n        {'id': 1, 'translation': {'en_test': 'Hello world', 'fr_test': 'Bonjour tout le monde'}},\n        {'id': 2, 'translation': {'en_test': 'How are you?', 'fr_test': 'Comment ça va ?'}}\n    ]\n\n    # Example config\n    config = {\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    tokenizer_src = get_or_build_tokenizer(config, dataset, \"en_test\")\n    tokenizer_tgt = get_or_build_tokenizer(config, dataset, \"fr_test\")\n\n    # Example usage\n    lang = 'fr_test'\n    bilingual_dataset = BilingualDataset(dataset, tokenizer_src, tokenizer_tgt, 'en_test', lang, 10)\n\n    # Create a DataLoader for batching the data\n    dataloader = DataLoader(bilingual_dataset, batch_size=1, shuffle=True)\n    \n    batch_num = 0\n    # Iterate over the DataLoader to print batches\n    for batch in dataloader:\n        print(\"Encoder Input:\", batch[\"encoder_input\"].shape)\n        print(\"Decoder Input:\", batch[\"decoder_input\"].shape)\n        print(\"Encoder Mask:\", batch[\"encoder_mask\"].shape)\n        print(\"Decoder Mask:\", batch[\"decoder_mask\"].shape)\n        print(\"Label:\", batch[\"label\"].shape)\n        print(\"Source Text:\", batch[\"src_text\"])\n        print(\"Target Text:\", batch[\"tgt_text\"])\n        print(\"---\")\n        batch_num += 1\n    \n    assert batch_num == len(dataset)//dataloader.batch_size\n    \ntest_BilingualDataset()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:42.010512Z","iopub.execute_input":"2024-06-18T13:18:42.010817Z","iopub.status.idle":"2024-06-18T13:18:42.085969Z","shell.execute_reply.started":"2024-06-18T13:18:42.010793Z","shell.execute_reply":"2024-06-18T13:18:42.085065Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tokenizer_en_test.json does not exists.\ntokenizer_fr_test.json does not exists.\nEncoder Input: torch.Size([1, 10])\nDecoder Input: torch.Size([1, 10])\nEncoder Mask: torch.Size([1, 1, 1, 10])\nDecoder Mask: torch.Size([1, 1, 10, 10])\nLabel: torch.Size([1, 10])\nSource Text: ['Hello world']\nTarget Text: ['Bonjour tout le monde']\n---\nEncoder Input: torch.Size([1, 10])\nDecoder Input: torch.Size([1, 10])\nEncoder Mask: torch.Size([1, 1, 1, 10])\nDecoder Mask: torch.Size([1, 1, 10, 10])\nLabel: torch.Size([1, 10])\nSource Text: ['How are you?']\nTarget Text: ['Comment ça va ?']\n---\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_ds(config):\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n    # build tokenizers\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n    # keep 90% for training and 10% for validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(\n        train_ds_raw, \n        tokenizer_src, \n        tokenizer_tgt, \n        config['lang_src'], \n        config['lang_tgt'], \n        config['seq_len']\n    )\n    val_ds = BilingualDataset(\n        val_ds_raw, \n        tokenizer_src, \n        tokenizer_tgt, \n        config['lang_src'], \n        config['lang_tgt'], \n        config['seq_len']\n    )\n\n    max_len_src = 0\n    max_len_tgt = 0\n    \n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_src']]).ids\n        \n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f\"Max length for source senstences: {max_len_src}\")\n    print(f\"Max length for target senstences: {max_len_tgt}\")\n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=False)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:42.089472Z","iopub.execute_input":"2024-06-18T13:18:42.090269Z","iopub.status.idle":"2024-06-18T13:18:42.099607Z","shell.execute_reply.started":"2024-06-18T13:18:42.090236Z","shell.execute_reply":"2024-06-18T13:18:42.098776Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def test_get_ds():\n    config = {\n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'seq_len': 500,\n        'batch_size': 32,\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    # Print some information about the datasets and tokenizers\n    print(f\"Train Dataset Size: {len(train_dataloader.dataset)}\")\n    print(f\"Validation Dataset Size: {len(val_dataloader.dataset)}\")\n    print(f\"Source Language Tokenizer Vocabulary Size: {len(tokenizer_src.get_vocab())}\")\n    print(f\"Target Language Tokenizer Vocabulary Size: {len(tokenizer_tgt.get_vocab())}\")\n\n    # Iterate over a few batches from the dataloaders to check the data\n    for batch in train_dataloader:\n        print(\"Encoder Input Shape:\", batch[\"encoder_input\"].shape)\n        print(\"Decoder Input Shape:\", batch[\"decoder_input\"].shape)\n        print(\"Encoder Mask Shape:\", batch[\"encoder_mask\"].shape)\n        print(\"Decoder Mask Shape:\", batch[\"decoder_mask\"].shape)\n        print(\"Label Shape:\", batch[\"label\"].shape)\n        print(\"---\")\n        break  # Stop after printing the first batch\n        \n    assert len(tokenizer_src.get_vocab()) > 1000 and len(tokenizer_tgt.get_vocab()) > 1000\n        \ntest_get_ds()   ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:18:42.100700Z","iopub.execute_input":"2024-06-18T13:18:42.101088Z","iopub.status.idle":"2024-06-18T13:19:01.797151Z","shell.execute_reply.started":"2024-06-18T13:18:42.101056Z","shell.execute_reply":"2024-06-18T13:19:01.796167Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5685fad15743869b3e970907f0be60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067a3ce0e14f410fbe2e6e1a82afc877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3238fcf07b40449bf5a45af3f40298"}},"metadata":{}},{"name":"stdout","text":"tokenizer_en.json does not exists.\ntokenizer_it.json does not exists.\nMax length for source senstences: 309\nMax length for target senstences: 309\nTrain Dataset Size: 29098\nValidation Dataset Size: 3234\nSource Language Tokenizer Vocabulary Size: 25138\nTarget Language Tokenizer Vocabulary Size: 30000\nEncoder Input Shape: torch.Size([32, 500])\nDecoder Input Shape: torch.Size([32, 500])\nEncoder Mask Shape: torch.Size([32, 1, 1, 500])\nDecoder Mask Shape: torch.Size([32, 1, 500, 500])\nLabel Shape: torch.Size([32, 500])\n---\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:19:01.798607Z","iopub.execute_input":"2024-06-18T13:19:01.799030Z","iopub.status.idle":"2024-06-18T13:19:01.804617Z","shell.execute_reply.started":"2024-06-18T13:19:01.798994Z","shell.execute_reply":"2024-06-18T13:19:01.803492Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def test_get_model():\n    config  = {\n        'seq_len': 500,\n        'd_model': 512,\n    }\n    vocab_src_len = 512\n    vocab_tgt_len = 512\n\n    model = get_model(config, vocab_src_len, vocab_tgt_len)\n    model\n    \ntest_get_model()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:19:01.805890Z","iopub.execute_input":"2024-06-18T13:19:01.806202Z","iopub.status.idle":"2024-06-18T13:19:02.627851Z","shell.execute_reply.started":"2024-06-18T13:19:01.806176Z","shell.execute_reply":"2024-06-18T13:19:02.627035Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def validation_loss(model, validation_ds, tokenizer_src, tokenizer_tgt):\n    with torch.no_grad():\n        pass\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:19:02.629013Z","iopub.execute_input":"2024-06-18T13:19:02.629301Z","iopub.status.idle":"2024-06-18T13:19:02.633749Z","shell.execute_reply.started":"2024-06-18T13:19:02.629276Z","shell.execute_reply":"2024-06-18T13:19:02.632786Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def custom_trainer(config, train_for_epochs):\n    # def train_model(config):\n    # Define the Device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"Using Device:\", device)\n\n    # Make sure the weights folder exists\n    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    model = get_model(\n        config, \n        tokenizer_src.get_vocab_size(), \n        tokenizer_tgt.get_vocab_size()\n    ).to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    # If the user specified a model to preload before training, load it\n    initial_epoch = 0\n    global_step = 0\n    preload = config['preload']\n    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n\n    if model_filename:\n        print(f\"Preloading model: {model_filename}\")\n        state = torch.load(model_filename)\n    #     /kaggle/working/opus_books_weights/tmodel_03.pt\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n    else:\n        print('No model to preload, starting from scratch')\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)    \n        \n    # Example tensors\n    # logits = torch.tensor([[2.0, 0.5, -0.5], [0.1, 1.0, -1.0], [0.2, 0.3, 0.8]], dtype=torch.float32)\n    # targets = torch.tensor([0, 1, 2], dtype=torch.int64)\n    # loss = loss_fn(logits, targets)\n    \n    for epoch in range(initial_epoch, initial_epoch + train_for_epochs):\n\n        torch.cuda.empty_cache()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n\n        for i, batch in tqdm(enumerate(batch_iterator)):\n            if i < 3630:\n                continue\n                \n            if i == 3637 or i == 3636:\n                print(batch['encoder_input'].shape)\n                print(batch['decoder_input'].shape)\n                print(batch['encoder_mask'].shape)\n                print(batch['decoder_mask'].shape)\n                \n            model.train()\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n\n            # Run the tensors through the encoder, decoder and the projection layer\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n\n            # Compare the output with the label\n            label = batch['label'].to(device) # (B, seq_len)\n\n            # Compute the loss using a simple cross entropy\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            \n\n            # Backpropagate the loss\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            \n            # Log the loss\n            print(f'epoch_{epoch}__global_step_{global_step}__batch_no_{i}__train loss_{loss.item()}__val loss_')\n\n            global_step += 1\n\n        # Save the model at the end of every epoch\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:19:02.634962Z","iopub.execute_input":"2024-06-18T13:19:02.635253Z","iopub.status.idle":"2024-06-18T13:19:02.652089Z","shell.execute_reply.started":"2024-06-18T13:19:02.635229Z","shell.execute_reply":"2024-06-18T13:19:02.651195Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"config = get_config()\ncustom_trainer(config, train_for_epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:19:02.653177Z","iopub.execute_input":"2024-06-18T13:19:02.653504Z","iopub.status.idle":"2024-06-18T13:21:35.824199Z","shell.execute_reply.started":"2024-06-18T13:19:02.653474Z","shell.execute_reply":"2024-06-18T13:21:35.822716Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Using Device: cuda\ntokenizer_en.json exists.\ntokenizer_it.json exists.\nMax length for source senstences: 309\nMax length for target senstences: 309\nNo model to preload, starting from scratch\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 00:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab3137c3fe5f45b6b319070c40f3ae36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ee92da1f0d4b54af33bb5d052a61a3"}},"metadata":{}},{"name":"stdout","text":"torch.Size([8, 350])\ntorch.Size([8, 350])\ntorch.Size([8, 1, 1, 350])\ntorch.Size([8, 1, 350, 350])\ntorch.Size([2, 350])\ntorch.Size([2, 350])\ntorch.Size([2, 1, 1, 350])\ntorch.Size([2, 1, 350, 350])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 01:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9b29a1802747458fc893b32124e343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9c2c95891648a3a705dc14d70647e5"}},"metadata":{}},{"name":"stdout","text":"torch.Size([8, 350])\ntorch.Size([8, 350])\ntorch.Size([8, 1, 1, 350])\ntorch.Size([8, 1, 350, 350])\ntorch.Size([2, 350])\ntorch.Size([2, 350])\ntorch.Size([2, 1, 1, 350])\ntorch.Size([2, 1, 350, 350])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 02:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9c0c7343b94781ac9e3e2a3c93d9ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77d1e2f84ab4dcaa69edb3f2f6d175d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcustom_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_for_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 49\u001b[0m, in \u001b[0;36mcustom_trainer\u001b[0;34m(config, train_for_epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     47\u001b[0m batch_iterator \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(batch_iterator)):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3630\u001b[39m:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mBilingualDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# add SOS and EOS to the source text\u001b[39;00m\n\u001b[1;32m     39\u001b[0m encoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     40\u001b[0m     [\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msos_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     ]   \n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     49\u001b[0m     [\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msos_token,\n\u001b[1;32m     51\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(dec_input_tokens, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m---> 52\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_num_padding_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     ]   \n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     57\u001b[0m     [\n\u001b[1;32m     58\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(dec_input_tokens, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     ]   \n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoder_input, \u001b[38;5;66;03m#seq_len\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: decoder_input, \u001b[38;5;66;03m#seq_len\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: tgt_text\n\u001b[1;32m     72\u001b[0m }\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 0\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:01:04.105939Z","iopub.status.idle":"2024-06-18T13:01:04.106345Z","shell.execute_reply.started":"2024-06-18T13:01:04.106142Z","shell.execute_reply":"2024-06-18T13:01:04.106158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-17T22:31:04.485358Z","iopub.execute_input":"2024-06-17T22:31:04.485717Z","iopub.status.idle":"2024-06-17T22:32:11.542327Z","shell.execute_reply.started":"2024-06-17T22:31:04.485688Z","shell.execute_reply":"2024-06-17T22:32:11.541114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}