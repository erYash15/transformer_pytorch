{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ATTENTION IS ALL YOU NEED!**\n",
    "\n",
    "## Language Translator\n",
    "\n",
    "Reference: [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/@umarjamilai)\n",
    "\n",
    "---\n",
    "\n",
    "Hello Readers,\n",
    "\n",
    "In this project, we are going to implement the transformer from the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook will guide you through the following steps:\n",
    "\n",
    "1. **Data Preparation:** Loading and preprocessing the English-Italian translation dataset.\n",
    "2. **Model Architecture:** Building the transformer model as described in the paper.\n",
    "3. **Training:** Training the transformer model on the dataset.\n",
    "4. **Evaluation:** Evaluating the model's performance on test data.\n",
    "5. **Inference:** Translating English sentences to Italian using the trained model.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "We will start with some helper functions, then usual imports, then directly jump into model architecture, then prepare the dataset for training, and finally perform inference. Variable names used in the code aligns with paper.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/model_arch.png\" alt=\"Image\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and print the number of trainable parameters in a model.\n",
    "\n",
    "    This function iterates over all parameters in the given model,\n",
    "    filters those that require gradients (trainable parameters), \n",
    "    and prints each parameter's count. Finally, it prints the total\n",
    "    count of trainable parameters.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model for which to count parameters.\n",
    "\n",
    "    Example:\n",
    "        model = YourModel()\n",
    "        count_parameters(model)\n",
    "    \"\"\"\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/input_embedding.png\" alt=\"Image\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Input Embeddings Class\n",
    "\n",
    "    This class defines the input embedding layer used in the transformer model.\n",
    "    It converts input tokens to their corresponding embeddings and scales them\n",
    "    by the square root of the model's dimensionality (d_model).\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimensionality of the embeddings.\n",
    "        vocab_size (int): The size of the vocabulary (i.e., the number of unique tokens).\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Computes the embeddings for the input tokens and scales them.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scaled embeddings for the input tokens.\n",
    "        \"\"\"\n",
    "        # Compute embeddings and scale them by sqrt(d_model)\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "d_model = 512  # Dimension of the model\n",
    "vocab_size = 25000  # Maximum length of the sequence\n",
    "input_tensor = torch.randint(low = 0, high = vocab_size-1, size = (10,50), dtype=torch.int) # 10 lines, 50 words per line\n",
    "input_embeddings = InputEmbeddings(d_model, vocab_size)\n",
    "input_embeddings(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800000\n",
      "______\n",
      "12800000\n"
     ]
    }
   ],
   "source": [
    "count_parameters(input_embeddings) # weights=> 512*25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Positional_encoding_1.png\" alt=\"Image\" width=\"1000\"/>\n",
    "<img src=\"images/Positional_encoding_2.png\" alt=\"Image\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding Class\n",
    "\n",
    "    This class adds positional encoding to the input embeddings to provide \n",
    "    the model with information about the relative or absolute position of \n",
    "    tokens in the sequence. The implementation includes a dropout layer \n",
    "    for regularization.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimensionality of the embeddings.\n",
    "        seq_len (int): The maximum length of the input sequences.\n",
    "        dropout (float): The dropout rate for regularization.\n",
    "        div_term_implementation (str): Specifies the method to compute the \n",
    "            division term. Options are 'original' or 'modified'.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Adds positional encoding to the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float, div_term_implementation: str = 'modified') -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a matrix of shape (seq_len, d_model) for positional encodings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len, 1) for position indices\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the division term based on the specified implementation\n",
    "        if div_term_implementation == 'original':\n",
    "            div_term = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        else:\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an extra dimension for batch size\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register the positional encoding matrix as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the positional encoding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor containing embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Input tensor with added positional encoding.\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input tensor and disable gradient computation\n",
    "        x = x + (self.pe[:, :x.shape[1],]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "d_model = 512  # Dimension of the model\n",
    "max_len = 5000  # Maximum length of the sequence\n",
    "pos_encoder = PositionalEncoding(d_model, max_len, dropout=0.2)\n",
    "\n",
    "# Dummy input tensor with shape (batch size, sequence length, d_model)\n",
    "x = torch.zeros(32, 100, d_model)\n",
    "\n",
    "# Apply positional encoding\n",
    "x = pos_encoder(x)\n",
    "print(x.shape)  # Should output: torch.Size([100, 32, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "     0\n"
     ]
    }
   ],
   "source": [
    "count_parameters(pos_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Layer_Normalization.png\" alt=\"Image\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization Class\n",
    "\n",
    "    This class applies layer normalization to the input tensor of batch. \n",
    "    Layer normalization normalizes the inputs across the features \n",
    "    (i.e., the last dimension) to have mean zero and variance one. \n",
    "    It also includes learnable parameters for scaling and bias.\n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in a model\n",
    "        eps (float): A small value to avoid division by zero during normalization. \n",
    "                     Default is 10**-6.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Applies layer normalization to the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))        # Learnable scaling parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features))         # Learnable bias parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized input tensor with learnable scaling and bias.\n",
    "        \"\"\"\n",
    "\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch, seq_len, 1)\n",
    "        std = x.std(dim=-1, keepdim=True) # (batch, seq_len, 1)\n",
    "        # Apply normalization, scaling, and bias\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([16, 500, 512])\n",
      "Output Shape: torch.Size([16, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch = 16\n",
    "seq_len = 500\n",
    "d_model = 512\n",
    "# Input tensor of shape (batch_size, sequence_length, d_model)\n",
    "x = torch.randn(batch, seq_len, d_model)\n",
    "# Instantiate the LayerNormalization class\n",
    "layer_norm = LayerNormalization(d_model)\n",
    "# Apply layer normalization\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   512\n",
      "   512\n",
      "______\n",
      "  1024\n"
     ]
    }
   ],
   "source": [
    "count_parameters(layer_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    FeedForward Block Class\n",
    "\n",
    "    This class defines a feedforward neural network block used in the \n",
    "    transformer model. It consists of two linear transformations with \n",
    "    a ReLU activation in between, and dropout for regularization.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimensionality of the input and output features.\n",
    "        d_ff (int): The dimensionality of the intermediate (hidden) features.\n",
    "        dropout (float): The dropout rate for regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Applies the feedforward block to the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear layer with input size d_model and output size d_ff\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Second linear layer with input size d_ff and output size d_model\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the feedforward block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (Batch, Seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape (Batch, Seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # Apply the first linear layer, ReLU activation, dropout, and then the second linear layer\n",
    "        # (Batch, Seq_len, d_model) -> (Batch, Seq_len, d_ff) -> (Batch, Seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([16, 500, 512])\n",
      "Output Shape: torch.Size([16, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_size = 16\n",
    "seq_len = 500\n",
    "d_model = 512\n",
    "\n",
    "# Input tensor of shape (batch_size, sequence_length, d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # Example input tensor\n",
    "\n",
    "# Instantiate the FeedForwardBlock class\n",
    "d_ff = 2048  # Dimension of the feedforward layer\n",
    "dropout = 0.1  # Dropout rate\n",
    "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "# Apply the feedforward block\n",
    "output = feed_forward_block(x)\n",
    "\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n",
      "  2048\n",
      "1048576\n",
      "   512\n",
      "______\n",
      "2099712\n"
     ]
    }
   ],
   "source": [
    "count_parameters(feed_forward_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26th March 10:32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Multi_head_attention.png\" alt=\"Image\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention Block Class\n",
    "\n",
    "    This class implements the multi-head attention mechanism as described \n",
    "    in the \"Attention Is All You Need\" paper. It allows the model to jointly \n",
    "    attend to information from different representation subspaces.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimensionality of the input and output features.\n",
    "        h (int): The number of attention heads.\n",
    "        dropout (float): The dropout rate for regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(q, k, v, mask): Applies multi-head attention to the input tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model%h == 0, \"d_model not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model//h\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model) #wq\n",
    "        self.w_k = nn.Linear(d_model, d_model) #wk\n",
    "        self.w_v = nn.Linear(d_model, d_model) #wv\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) #wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod \n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        \"\"\"\n",
    "        Compute the attention scores and apply the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor of shape (Batch, h, Seq_Len, d_k).\n",
    "            key (torch.Tensor): Key tensor of shape (Batch, h, Seq_Len, d_k).\n",
    "            value (torch.Tensor): Value tensor of shape (Batch, h, Seq_Len, d_k).\n",
    "            mask (torch.Tensor): Mask tensor to avoid attending to certain positions.\n",
    "            dropout (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying attention.\n",
    "            torch.Tensor: The attention scores.\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # (Batch, h, seq_len, d_k) -> (Batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # very low value (indicateing -inf) to positions where mask == 0\n",
    "            attention_scores.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # (Batch, h, seq_len, seq_len)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch. h. seq_len, seq_len) -> (batch, h, seq_len, d_k)\n",
    "        # also resturn attention scores used for visualization\n",
    "        return (attention_scores @ value), attention_scores        \n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head attention block.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape (Batch, Seq_Len, d_model).\n",
    "            k (torch.Tensor): Key tensor of shape (Batch, Seq_Len, d_model).\n",
    "            v (torch.Tensor): Value tensor of shape (Batch, Seq_Len, d_model).\n",
    "            mask (torch.Tensor): Mask tensor to avoid attending to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying multi-head attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply linear layers to get query, key, and value tensors\n",
    "        query = self.w_q(q) # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, d_model)\n",
    "        key = self.w_k(k) # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, d_model)\n",
    "        value = self.w_v(v) # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, d_model)\n",
    "\n",
    "        # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, h, d_k) -> (Batch, h, Seq_Len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Apply the attention mechanism\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reshape and transpose back to the original shape\n",
    "        # (Batch, h, seq_len, d_k) -> (Batch, seq_len, h, d_k) -> (Batch, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h*self.d_k)\n",
    "\n",
    "        # Apply the final linear layer (Batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "h = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# Create a random tensor with shape (Batch, Seq_Len, d_model)\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "    \n",
    "# Create a mask tensor to avoid attending to certain positions\n",
    "mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "mask[:, :, 2:, :] = 0  # Masking out the last three positions\n",
    "\n",
    "# Create a multi-head attention block\n",
    "mha_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "\n",
    "# Apply the multi-head attention block\n",
    "output = mha_block(q, k, v, mask)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   256\n",
      "    16\n",
      "   256\n",
      "    16\n",
      "   256\n",
      "    16\n",
      "   256\n",
      "    16\n",
      "______\n",
      "  1088\n"
     ]
    }
   ],
   "source": [
    "count_parameters(mha_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Residual_network.png\" alt=\"Image\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Connection with Layer Normalization.\n",
    "\n",
    "    This module adds a residual connection around any given sublayer \n",
    "    with layer normalization and dropout applied before the residual \n",
    "    connection is added.\n",
    "\n",
    "    Args:\n",
    "        features (int): Number of input features\n",
    "        dropout (float): Dropout rate to be applied after the sublayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            sublayer (Callable): A sublayer function or module.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the residual connection and dropout.\n",
    "        \"\"\"\n",
    "        # Norm and Add\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([16, 500, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "batch_size = 16\n",
    "seq_len = 500\n",
    "d_model = 512\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create a random tensor with shape (Batch, Seq_Len, d_model)\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Define a simple sublayer function for demonstration\n",
    "class SimpleSublayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.linear(x))\n",
    "\n",
    "sublayer = SimpleSublayer(d_model)\n",
    "\n",
    "# Create a residual connection block\n",
    "residual_connection = ResidualConnection(d_model, dropout_rate)\n",
    "\n",
    "# Apply the residual connection block with the sublayer\n",
    "output = residual_connection(x, sublayer)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   512\n",
      "   512\n",
      "______\n",
      "  1024\n"
     ]
    }
   ],
   "source": [
    "count_parameters(residual_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Encoder.png\" alt=\"Image\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Encoder Block for a Transformer model.\n",
    "\n",
    "        This block consists of a self-attention mechanism followed by a feed-forward network, \n",
    "        with residual connections and layer normalization applied to each sublayer.\n",
    "\n",
    "        Args:\n",
    "            features (int): Number of input Features\n",
    "            self_attention_block (MultiHeadAttentionBlock): Multi-head self-attention mechanism.\n",
    "            feed_forward_block (FeedForwardBlock): Feed-forward network.\n",
    "            dropout (float): Dropout rate to be applied after each sublayer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            src_mask (torch.Tensor): Source mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying self-attention, feed-forward network, and residual connections.\n",
    "        \"\"\"\n",
    "        # Apply the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))  # try without lambda\n",
    "\n",
    "        # Apply the second residual connection with the feed-forward block\n",
    "        x = self.residual_connections[1](x, lambda x: self.feed_forward_block(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "h = 4\n",
    "d_ff = 32\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create random tensors for input and mask\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "# Create a mask tensor to avoid attending to certain positions\n",
    "mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "mask[:, :, 2:, :] = 0  # Masking out the last three positions\n",
    "\n",
    "# Initialize self-attention block and feed-forward block\n",
    "self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout_rate)\n",
    "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout_rate)\n",
    "\n",
    "# Initialize the encoder block\n",
    "encoder_block = EncoderBlock(d_model, self_attention_block, feed_forward_block, dropout_rate)\n",
    "\n",
    "# Apply the encoder block\n",
    "output = encoder_block(x, mask)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block for a Transformer model.\n",
    "\n",
    "    This block consists of a stack of layers (e.g., multi-head self-attention and feed-forward networks)\n",
    "    followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Encoder with a list of layers and a layer normalization module.\n",
    "\n",
    "        Args:\n",
    "            features (int): Number of input features\n",
    "            layers (nn.ModuleList): List of layers to be included in the encoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)  # Initialize layer normalization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor): Mask tensor of shape (batch_size, 1, 1, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying all layers and layer normalization.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)  # Apply each layer in the list to the input\n",
    "        return self.norm(x)  # Apply layer normalization to the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "h = 4\n",
    "d_ff = 32\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create random tensors for input and mask\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "# Create a mask tensor to avoid attending to certain positions\n",
    "mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "mask[:, :, 2:, :] = 0  # Masking out the last three positions\n",
    "\n",
    "# Initialize self-attention block and feed-forward block\n",
    "self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout_rate)\n",
    "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout_rate)\n",
    "\n",
    "# Initialize the encoder block\n",
    "encoder_blocks = nn.ModuleList()\n",
    "for i in range(10):\n",
    "    encoder_blocks.append(EncoderBlock(d_model, self_attention_block, feed_forward_block, dropout_rate))\n",
    "\n",
    "encoder = Encoder(d_model, encoder_blocks)\n",
    "\n",
    "# Apply the encoder block\n",
    "output = encoder(x, mask)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    " # count_parameters(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder_Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Decoder_Block.png\" alt=\"Image\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block for a Transformer model.\n",
    "\n",
    "    This block consists of a self-attention mechanism, a cross-attention mechanism (attending to encoder output),\n",
    "    and a feed-forward network. Each sublayer is followed by a residual connection and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            features: int,\n",
    "            self_attention_block: MultiHeadAttentionBlock, \n",
    "            cross_attention_block: MultiHeadAttentionBlock,\n",
    "            feed_forward_block: FeedForwardBlock,\n",
    "            dropout: float = 0.1\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DecoderBlock with self-attention, cross-attention, feed-forward blocks,\n",
    "        and residual connections.\n",
    "\n",
    "        Args:\n",
    "            self_attention_block (MultiHeadAttentionBlock): Multi-head self-attention mechanism.\n",
    "            cross_attention_block (MultiHeadAttentionBlock): Cross-attention mechanism to attend to encoder output.\n",
    "            feed_forward_block (FeedForwardBlock): Feed-forward network.\n",
    "            dropout (float): Dropout rate to be applied after each sublayer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block  # Self-attention mechanism\n",
    "        self.cross_attention_block = cross_attention_block  # Cross-attention mechanism\n",
    "        self.feed_forward_block = feed_forward_block  # Feed-forward network\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])  # Residual connections with dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            encoder_output (torch.Tensor): Encoder output tensor of shape (batch_size, seq_len, d_model).\n",
    "            src_mask (torch.Tensor): Source mask tensor.\n",
    "            tgt_mask (torch.Tensor): Target mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying self-attention, cross-attention, feed-forward network, and residual connections.\n",
    "        \"\"\"\n",
    "        # Apply the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "        # Apply the second residual connection with the cross-attention block\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        # Apply the third residual connection with the feed-forward block\n",
    "        x = self.residual_connections[2](x, lambda x: self.feed_forward_block(x))\n",
    "\n",
    "        return x  # Return the final output tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "d_model = 512  # Dimension of the model\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048  # Dimension of the feed-forward layer\n",
    "dropout = 0.1  # Dropout rate\n",
    "seq_len = 10  # Sequence length\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Create instances of the attention and feed-forward blocks\n",
    "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "cross_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "# Create an instance of the DecoderBlock\n",
    "decoder_block = DecoderBlock(d_model, self_attention_block, cross_attention_block, feed_forward_block, dropout)\n",
    "\n",
    "# Dummy input tensors\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # Input tensor\n",
    "encoder_output = torch.randn(batch_size, seq_len, d_model)  # Encoder output tensor\n",
    "src_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # Source mask tensor\n",
    "tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # Target mask tensor\n",
    "\n",
    "# Forward pass through the DecoderBlock\n",
    "output = decoder_block(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for a Transformer model.\n",
    "\n",
    "    This decoder consists of a stack of decoder layers, each containing self-attention, \n",
    "    cross-attention, and feed-forward networks with residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Decoder with a list of layers and a layer normalization module.\n",
    "\n",
    "        Args:\n",
    "            features (int): Number of input feautres for layewr normalization\n",
    "            layers (nn.ModuleList): List of decoder layers to be included in the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers  # List of decoder layers\n",
    "        self.norm = LayerNormalization(features)  # Initialize layer normalization\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            encoder_output (torch.Tensor): Encoder output tensor of shape (batch_size, seq_len, d_model).\n",
    "            src_mask (torch.Tensor): Source mask tensor.\n",
    "            tgt_mask (torch.Tensor): Target mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying all decoder layers and layer normalization.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)  # Apply each decoder layer to the input\n",
    "\n",
    "        return self.norm(x)  # Apply layer normalization to the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "d_model = 512  # Dimension of the model\n",
    "num_heads = 8  # Number of attention heads\n",
    "d_ff = 2048  # Dimension of the feed-forward layer\n",
    "dropout = 0.1  # Dropout rate\n",
    "seq_len = 10  # Sequence length\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Create instances of the attention and feed-forward blocks\n",
    "self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "cross_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "# Create a list of decoder blocks\n",
    "decoder_blocks = nn.ModuleList([\n",
    "    DecoderBlock(d_model, self_attention_block, cross_attention_block, feed_forward_block, dropout)\n",
    "    for _ in range(6)  # Number of decoder layers\n",
    "])\n",
    "\n",
    "# Create an instance of the Decoder\n",
    "decoder = Decoder(d_model, decoder_blocks)\n",
    "\n",
    "# Dummy input tensors\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # Input tensor\n",
    "encoder_output = torch.randn(batch_size, seq_len, d_model)  # Encoder output tensor\n",
    "src_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # Source mask tensor\n",
    "tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # Target mask tensor\n",
    "\n",
    "# Forward pass through the Decoder\n",
    "output = decoder(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection layer for a Transformer model.\n",
    "\n",
    "    This layer projects the hidden states from the model's d_model dimensionality\n",
    "    to the vocabulary size dimensionality and applies a log softmax function.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ProjectionLayer with a linear projection layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model's hidden states.\n",
    "            vocab_size (int): Size of the vocabulary (number of target classes).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)  # Linear layer to project to vocabulary size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the projection layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, vocab_size) with log softmax applied.\n",
    "        \"\"\"\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)  # Apply linear projection and log softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "d_model = 512  # Dimensionality of the model's hidden states\n",
    "vocab_size = 10000  # Size of the vocabulary\n",
    "seq_len = 10  # Sequence length\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Create an instance of the ProjectionLayer\n",
    "projection_layer = ProjectionLayer(d_model, vocab_size)\n",
    "\n",
    "# Dummy input tensor of shape (batch_size, seq_len, d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass through the ProjectionLayer\n",
    "output = projection_layer(x)\n",
    "\n",
    "# Output tensor should have shape (batch_size, seq_len, vocab_size)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/model_arch.png\" alt=\"Image\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        src_seq_len: int,\n",
    "        tgt_seq_len: int,\n",
    "        d_model: int = 512,\n",
    "        N: int = 6,\n",
    "        h: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        d_ff: int = 2048\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "        self.tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "        # Positional encodings\n",
    "        self.src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "        self.tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_blocks = []\n",
    "        for _ in range(N):\n",
    "            encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "            encoder_blocks.append(encoder_block)\n",
    "        self.encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "\n",
    "        # Decoder\n",
    "        decoder_blocks = []\n",
    "        for _ in range(N):\n",
    "            decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "            decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "            decoder_blocks.append(decoder_block)\n",
    "        self.decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "        # Projection layer\n",
    "        self.projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(encoder_output, src_mask, tgt, tgt_mask)\n",
    "        output = self.project(decoder_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transformer forward pass test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_forward_pass():\n",
    "    # Hyperparameters\n",
    "    src_vocab_size = 30000\n",
    "    tgt_vocab_size = 25000\n",
    "    src_seq_len = 100\n",
    "    tgt_seq_len = 80\n",
    "    d_model = 512\n",
    "    batch_size = 2\n",
    "\n",
    "    # Instantiate the Transformer\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        src_seq_len=src_seq_len,\n",
    "        tgt_seq_len=tgt_seq_len,\n",
    "        d_model=d_model,\n",
    "        N=6,\n",
    "        h=8,\n",
    "        dropout=0.1,\n",
    "        d_ff=1024\n",
    "    )\n",
    "\n",
    "    # Create dummy inputs\n",
    "    src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))  # [batch_size, src_seq_len]\n",
    "    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))  # [batch_size, tgt_seq_len]\n",
    "\n",
    "    # Create correct masks\n",
    "    src_mask = torch.ones((batch_size, 1, 1, src_seq_len))  # [batch_size, 1, 1, src_seq_len]\n",
    "    tgt_mask = torch.ones((batch_size, 1, tgt_seq_len, tgt_seq_len))  # [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    # Check output shape: [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "    assert out.shape == (batch_size, tgt_seq_len, tgt_vocab_size), f\"Unexpected output shape: {out.shape}\"\n",
    "\n",
    "    print(\" Transformer forward pass test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_transformer_forward_pass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # -------------- Dummy Dataset --------------\n",
    "# class DummyTranslationDataset(Dataset):\n",
    "#     def __init__(self, src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, num_samples=10000):\n",
    "#         self.src_vocab_size = src_vocab_size\n",
    "#         self.tgt_vocab_size = tgt_vocab_size\n",
    "#         self.src_seq_len = src_seq_len\n",
    "#         self.tgt_seq_len = tgt_seq_len\n",
    "#         self.num_samples = num_samples\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         src = torch.randint(0, self.src_vocab_size, (self.src_seq_len,))\n",
    "#         tgt = torch.randint(0, self.tgt_vocab_size, (self.tgt_seq_len,))\n",
    "#         return src, tgt\n",
    "\n",
    "# # -------------- Mask Functions --------------\n",
    "# def create_src_mask(src):\n",
    "#     batch_size, src_len = src.shape\n",
    "#     src_mask = torch.ones((batch_size, 1, 1, src_len), device=src.device)\n",
    "#     return src_mask\n",
    "\n",
    "# def create_tgt_mask(tgt):\n",
    "#     batch_size, tgt_len = tgt.shape\n",
    "#     nopeak_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "#     tgt_mask = nopeak_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, tgt_len, tgt_len)\n",
    "#     return tgt_mask\n",
    "\n",
    "\n",
    "# # -------------- Hyperparameters --------------\n",
    "# src_vocab_size = 10000  # English vocab\n",
    "# tgt_vocab_size = 12000  # French vocab\n",
    "# src_seq_len = 30\n",
    "# tgt_seq_len = 30\n",
    "# batch_size = 64\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1e-4\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # -------------- Model, Loss, Optimizer --------------\n",
    "# model = Transformer(\n",
    "#     src_vocab_size=src_vocab_size,\n",
    "#     tgt_vocab_size=tgt_vocab_size,\n",
    "#     src_seq_len=src_seq_len,\n",
    "#     tgt_seq_len=tgt_seq_len,\n",
    "#     d_model=512,\n",
    "#     N=6,\n",
    "#     h=8,\n",
    "#     dropout=0.1,\n",
    "#     d_ff=1024\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)  # assume padding=0\n",
    "\n",
    "# # -------------- Data Loaders --------------\n",
    "# train_dataset = DummyTranslationDataset(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # -------------- Training Loop --------------\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "#     for src, tgt in loop:\n",
    "#         src = src.to(device)\n",
    "#         tgt = tgt.to(device)\n",
    "\n",
    "#         src_mask = create_src_mask(src)\n",
    "#         tgt_mask = create_tgt_mask(tgt)\n",
    "\n",
    "#         # Prepare inputs and targets\n",
    "#         tgt_input = tgt[:, :-1]    # Remove last token for input\n",
    "#         tgt_output = tgt[:, 1:]    # Remove first token for output\n",
    "\n",
    "#         tgt_mask = create_tgt_mask(tgt_input)\n",
    "\n",
    "#         preds = model(src, tgt_input, src_mask, tgt_mask)  # (batch_size, tgt_seq_len-1, tgt_vocab_size)\n",
    "\n",
    "#         preds = preds.reshape(-1, preds.shape[-1])  # Flatten for loss\n",
    "#         tgt_output = tgt_output.reshape(-1)         # Flatten target\n",
    "\n",
    "#         loss = criterion(preds, tgt_output)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# print(\" Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
