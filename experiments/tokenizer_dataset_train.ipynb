{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Model Training Notebook\n\nThis notebook demonstrates how to train a transformer model in the Kaggle environment. We will go through the steps of setting up the environment, loading data, preprocessing, training the model, and evaluating its performance.\n\nHowever, Architecture is available in the github repo: **eryash15/transformer_pytorch**\n\n## Kaggle\nKaggle provides the perfect environment for training the model with\n\n## Setup\n\nFirst, let's load the code repository and import the necessary libraries.","metadata":{}},{"cell_type":"markdown","source":"## Clone the tranformer repositry (it doesn't require any external data)","metadata":{}},{"cell_type":"code","source":"! git clone https://github.com/erYash15/transformer_pytorch.git","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:38:45.020863Z","iopub.execute_input":"2024-06-26T21:38:45.021901Z","iopub.status.idle":"2024-06-26T21:38:46.780624Z","shell.execute_reply.started":"2024-06-26T21:38:45.021860Z","shell.execute_reply":"2024-06-26T21:38:46.779517Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'transformer_pytorch'...\nremote: Enumerating objects: 102, done.\u001b[K\nremote: Counting objects: 100% (102/102), done.\u001b[K\nremote: Compressing objects: 100% (79/79), done.\u001b[K\nremote: Total 102 (delta 24), reused 76 (delta 17), pack-reused 0\u001b[K\nReceiving objects: 100% (102/102), 2.86 MiB | 27.35 MiB/s, done.\nResolving deltas: 100% (24/24), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nfrom pathlib import Path\n\nfrom torchmetrics.text import BLEUScore\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:38:46.783587Z","iopub.execute_input":"2024-06-26T21:38:46.784384Z","iopub.status.idle":"2024-06-26T21:39:04.131504Z","shell.execute_reply.started":"2024-06-26T21:38:46.784325Z","shell.execute_reply":"2024-06-26T21:39:04.130657Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-26 21:38:54.497750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 21:38:54.497882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 21:38:54.624185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append('../scripts')\nsys.path.append('/kaggle/working/transformer_pytorch/scripts')\n\n\nfrom Transformer import build_transformer\nfrom utils import count_parameters\nfrom config import get_config, get_weights_file_path, latest_weights_file_path","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.133109Z","iopub.execute_input":"2024-06-26T21:39:04.133698Z","iopub.status.idle":"2024-06-26T21:39:04.155122Z","shell.execute_reply.started":"2024-06-26T21:39:04.133669Z","shell.execute_reply":"2024-06-26T21:39:04.154313Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define the Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using Device:\", device)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.156295Z","iopub.execute_input":"2024-06-26T21:39:04.156631Z","iopub.status.idle":"2024-06-26T21:39:04.200646Z","shell.execute_reply.started":"2024-06-26T21:39:04.156604Z","shell.execute_reply":"2024-06-26T21:39:04.199632Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using Device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    \"\"\"Generates the sentence\"\"\"\n    for item in ds:\n        yield item['translation'][lang]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.203259Z","iopub.execute_input":"2024-06-26T21:39:04.203628Z","iopub.status.idle":"2024-06-26T21:39:04.213985Z","shell.execute_reply.started":"2024-06-26T21:39:04.203602Z","shell.execute_reply":"2024-06-26T21:39:04.213072Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Example\ndef test_get_all_sentences():\n    input_dataset = [\n        {'id': 1, 'translation': {'en': 'Hello', 'fr': 'Bonjour'}},\n        {'id': 2, 'translation': {'en': 'Goodbye', 'fr': 'Au revoir'}}\n    ]    \n    expected_output = ['Bonjour', 'Au revoir'] \n\n    result_output = []\n\n    lang = 'fr'\n    for sentence in get_all_sentences(input_dataset, lang):\n        result_output.append(sentence)\n    \n    assert expected_output == result_output \n    print(\"test_get_all_sentences: test success\")\n    \ntest_get_all_sentences()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.215145Z","iopub.execute_input":"2024-06-26T21:39:04.215455Z","iopub.status.idle":"2024-06-26T21:39:04.226414Z","shell.execute_reply.started":"2024-06-26T21:39:04.215430Z","shell.execute_reply":"2024-06-26T21:39:04.225577Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"test_get_all_sentences: test success\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        print(f\"{tokenizer_path} does not exists.\")\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        print(f\"{tokenizer_path} exists.\")\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.227514Z","iopub.execute_input":"2024-06-26T21:39:04.227811Z","iopub.status.idle":"2024-06-26T21:39:04.238477Z","shell.execute_reply.started":"2024-06-26T21:39:04.227787Z","shell.execute_reply":"2024-06-26T21:39:04.237556Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Example\ndef test_get_or_build_tokenizer():\n\n    config = {\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    dataset = [\n        {'id': 1, 'translation': {'en': 'Hello world', 'fr': 'Bonjour tout le monde'}},\n        {'id': 2, 'translation': {'en': 'How are you?', 'fr': 'Comment ça va ?'}}\n    ]\n\n    lang = 'fr'\n    tokenizer_not_exist = get_or_build_tokenizer(config, dataset, lang)\n    tokenizer = get_or_build_tokenizer(config, dataset, lang)\n    \n    assert tokenizer.encode(\"Bonjour tout le monde yash\").tokens == ['Bonjour', 'tout', 'le', 'monde', '[UNK]']\n    assert tokenizer.encode(\"Bonjour tout le monde yash\").ids == [5, 9, 7, 8, 0]\n    \n    print(\"test_get_or_build_tokenizer: test success\")\n    \ntest_get_or_build_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.239426Z","iopub.execute_input":"2024-06-26T21:39:04.239695Z","iopub.status.idle":"2024-06-26T21:39:04.276038Z","shell.execute_reply.started":"2024-06-26T21:39:04.239672Z","shell.execute_reply":"2024-06-26T21:39:04.275118Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tokenizer_fr.json does not exists.\ntokenizer_fr.json exists.\ntest_get_or_build_tokenizer: test success\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n\n        # since all these token for both lang remain same we can use tokenizer_tgt or tokenizer_src\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    @staticmethod\n    def causal_mask(size):\n        mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n        return mask == 0\n\n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, index):\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # SOS and EOS\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # no EOS or SOS\n\n        if enc_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n        \n        # add SOS and EOS to the source text\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]   \n        )\n\n        return {\n            \"encoder_input\": encoder_input, #seq_len\n            \"decoder_input\": decoder_input, #seq_len\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & BilingualDataset.causal_mask(decoder_input.size(0)), # (1,seq_len) & (1, seq_len, seq_len)\n            \"label\": label,\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text\n        }","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.279123Z","iopub.execute_input":"2024-06-26T21:39:04.279425Z","iopub.status.idle":"2024-06-26T21:39:04.294447Z","shell.execute_reply.started":"2024-06-26T21:39:04.279398Z","shell.execute_reply":"2024-06-26T21:39:04.293471Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Example \ndef test_BilingualDataset():\n    dataset = [\n        {'id': 1, 'translation': {'en_test': 'Hello world', 'fr_test': 'Bonjour tout le monde'}},\n        {'id': 2, 'translation': {'en_test': 'How are you?', 'fr_test': 'Comment ça va ?'}}\n    ]\n\n    # Example config\n    config = {\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    tokenizer_src = get_or_build_tokenizer(config, dataset, \"en_test\")\n    tokenizer_tgt = get_or_build_tokenizer(config, dataset, \"fr_test\")\n\n    # Example usage\n    lang = 'fr_test'\n    bilingual_dataset = BilingualDataset(dataset, tokenizer_src, tokenizer_tgt, 'en_test', lang, 10)\n\n    # Create a DataLoader for batching the data\n    dataloader = DataLoader(bilingual_dataset, batch_size=1, shuffle=True)\n    \n    batch_num = 0\n    # Iterate over the DataLoader to print batches\n    for batch in dataloader:\n        print(\"Encoder Input:\", batch[\"encoder_input\"].shape)\n        print(\"Decoder Input:\", batch[\"decoder_input\"].shape)\n        print(\"Encoder Mask:\", batch[\"encoder_mask\"].shape)\n        print(\"Decoder Mask:\", batch[\"decoder_mask\"].shape)\n        print(\"Label:\", batch[\"label\"].shape)\n        print(\"Source Text:\", batch[\"src_text\"])\n        print(\"Target Text:\", batch[\"tgt_text\"])\n        print(\"---\")\n        batch_num += 1\n    \n    assert batch_num == len(dataset)//dataloader.batch_size\n    \ntest_BilingualDataset()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.295675Z","iopub.execute_input":"2024-06-26T21:39:04.295966Z","iopub.status.idle":"2024-06-26T21:39:04.375943Z","shell.execute_reply.started":"2024-06-26T21:39:04.295941Z","shell.execute_reply":"2024-06-26T21:39:04.374995Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tokenizer_en_test.json does not exists.\ntokenizer_fr_test.json does not exists.\nEncoder Input: torch.Size([1, 10])\nDecoder Input: torch.Size([1, 10])\nEncoder Mask: torch.Size([1, 1, 1, 10])\nDecoder Mask: torch.Size([1, 1, 10, 10])\nLabel: torch.Size([1, 10])\nSource Text: ['How are you?']\nTarget Text: ['Comment ça va ?']\n---\nEncoder Input: torch.Size([1, 10])\nDecoder Input: torch.Size([1, 10])\nEncoder Mask: torch.Size([1, 1, 1, 10])\nDecoder Mask: torch.Size([1, 1, 10, 10])\nLabel: torch.Size([1, 10])\nSource Text: ['Hello world']\nTarget Text: ['Bonjour tout le monde']\n---\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_ds(config):\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n    # build tokenizers\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n    # keep 90% for training and 10% for validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(\n        train_ds_raw, \n        tokenizer_src, \n        tokenizer_tgt, \n        config['lang_src'], \n        config['lang_tgt'], \n        config['seq_len']\n    )\n    val_ds = BilingualDataset(\n        val_ds_raw, \n        tokenizer_src, \n        tokenizer_tgt, \n        config['lang_src'], \n        config['lang_tgt'], \n        config['seq_len']\n    )\n\n    max_len_src = 0\n    max_len_tgt = 0\n    \n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_src']]).ids\n        \n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f\"Max length for source senstences: {max_len_src}\")\n    print(f\"Max length for target senstences: {max_len_tgt}\")\n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=False)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.377399Z","iopub.execute_input":"2024-06-26T21:39:04.377739Z","iopub.status.idle":"2024-06-26T21:39:04.387775Z","shell.execute_reply.started":"2024-06-26T21:39:04.377712Z","shell.execute_reply":"2024-06-26T21:39:04.386647Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def test_get_ds():\n    config = {\n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'seq_len': 500,\n        'batch_size': 32,\n        'tokenizer_file': 'tokenizer_{0}.json'\n    }\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    # Print some information about the datasets and tokenizers\n    print(f\"Train Dataset Size: {len(train_dataloader.dataset)}\")\n    print(f\"Validation Dataset Size: {len(val_dataloader.dataset)}\")\n    print(f\"Source Language Tokenizer Vocabulary Size: {len(tokenizer_src.get_vocab())}\")\n    print(f\"Target Language Tokenizer Vocabulary Size: {len(tokenizer_tgt.get_vocab())}\")\n\n    # Iterate over a few batches from the dataloaders to check the data\n    for batch in train_dataloader:\n        print(\"Encoder Input Shape:\", batch[\"encoder_input\"].shape)\n        print(\"Decoder Input Shape:\", batch[\"decoder_input\"].shape)\n        print(\"Encoder Mask Shape:\", batch[\"encoder_mask\"].shape)\n        print(\"Decoder Mask Shape:\", batch[\"decoder_mask\"].shape)\n        print(\"Label Shape:\", batch[\"label\"].shape)\n        print(\"---\")\n        break  # Stop after printing the first batch\n        \n    assert len(tokenizer_src.get_vocab()) > 1000 and len(tokenizer_tgt.get_vocab()) > 1000\n        \ntest_get_ds()   ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:04.388935Z","iopub.execute_input":"2024-06-26T21:39:04.389211Z","iopub.status.idle":"2024-06-26T21:39:16.732618Z","shell.execute_reply.started":"2024-06-26T21:39:04.389187Z","shell.execute_reply":"2024-06-26T21:39:16.731655Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"612e7f176e354ae8aabcee9edd5524cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f82bde53a45f447b9b4d00a13eb1c7c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3284ffbdea4447b593e7d6060075cc5a"}},"metadata":{}},{"name":"stdout","text":"tokenizer_en.json does not exists.\ntokenizer_it.json does not exists.\nMax length for source senstences: 309\nMax length for target senstences: 309\nTrain Dataset Size: 29098\nValidation Dataset Size: 3234\nSource Language Tokenizer Vocabulary Size: 25138\nTarget Language Tokenizer Vocabulary Size: 30000\nEncoder Input Shape: torch.Size([32, 500])\nDecoder Input Shape: torch.Size([32, 500])\nEncoder Mask Shape: torch.Size([32, 1, 1, 500])\nDecoder Mask Shape: torch.Size([32, 1, 500, 500])\nLabel Shape: torch.Size([32, 500])\n---\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:16.733865Z","iopub.execute_input":"2024-06-26T21:39:16.734158Z","iopub.status.idle":"2024-06-26T21:39:16.739194Z","shell.execute_reply.started":"2024-06-26T21:39:16.734132Z","shell.execute_reply":"2024-06-26T21:39:16.738058Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def test_get_model():\n    config  = {\n        'seq_len': 500,\n        'd_model': 512,\n    }\n    vocab_src_len = 512\n    vocab_tgt_len = 512\n\n    model = get_model(config, vocab_src_len, vocab_tgt_len)\n    model\n    \ntest_get_model()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:16.742535Z","iopub.execute_input":"2024-06-26T21:39:16.742825Z","iopub.status.idle":"2024-06-26T21:39:17.617804Z","shell.execute_reply.started":"2024-06-26T21:39:16.742799Z","shell.execute_reply":"2024-06-26T21:39:17.616857Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def causal_mask(size):\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len):\n    \"\"\"\n    Performs greedy decoding for sequence generation.\n\n    Args:\n        model: The sequence-to-sequence model to use for decoding.\n        source: The input sequence to encode.\n        source_mask: The mask for the source sequence.\n        tokenizer_src: The tokenizer for the source language.\n        tokenizer_tgt: The tokenizer for the target language.\n        max_len: The maximum length for the generated sequence.\n        device: The device to perform the computations on.\n\n    Returns:\n        torch.Tensor: The generated sequence.\n    \"\"\"\n    # Get the special tokens' IDs\n    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n\n    # Encode the source sequence\n    encoder_output = model.encode(source, source_mask)\n\n    # Initialize the decoder input with the start-of-sequence token\n    decoder_input = torch.tensor([[sos_idx]], dtype=source_mask.dtype, device=device)\n\n    while True:\n        # Check if the maximum length is reached\n        if decoder_input.size(1) >= max_len:\n            break\n\n        # Create a causal mask for the decoder input\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        # Decode the current sequence\n        decoder_output = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # Project the decoder output to the vocabulary size and get the next token\n        logits = model.project(decoder_output[:, -1])\n        next_word = torch.argmax(logits, dim=1).item()\n\n        # Append the next token to the decoder input\n        next_word_tensor = torch.tensor([[next_word]], dtype=source.dtype, device=device)\n        decoder_input = torch.cat([decoder_input, next_word_tensor], dim=1)\n\n        # Stop if the end-of-sequence token is generated\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:17.619193Z","iopub.execute_input":"2024-06-26T21:39:17.619988Z","iopub.status.idle":"2024-06-26T21:39:17.630600Z","shell.execute_reply.started":"2024-06-26T21:39:17.619953Z","shell.execute_reply":"2024-06-26T21:39:17.629710Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Example to test greedy_decode\nclass DummySeq2SeqModel:\n    def encode(self, source, source_mask):\n        # Fake encoding, in practice use your actual model\n        return torch.randn(1, source.size(1), 768)\n    \n    def decode(self, encoder_output, source_mask, decoder_input, decoder_mask):\n        # Fake decoding, in practice use your actual model\n        return torch.randn(1, decoder_input.size(1), 768)\n    \n    def project(self, decoder_output):\n        # Fake projection, in practice use your actual model\n        vocab_size = 30522  # Assume BERT tokenizer vocab size\n        return torch.randn(decoder_output.size(0), vocab_size)\n\nconfig = get_config()\n\n# Load pre-trained tokenizers\ntokenizer_src = get_or_build_tokenizer(config, None, config['lang_src'])\ntokenizer_tgt = get_or_build_tokenizer(config, None, config['lang_tgt'])\n    \n# Example input sequence\nsource_text = \"Hello, how are you?\"\nsource = torch.tensor(tokenizer_src.encode(source_text).ids).unsqueeze(0)\n\nsource_mask = torch.ones_like(source).to(device)\n\n# Instantiate the dummy model\nmodel = DummySeq2SeqModel()\n\n# Define other parameters\nmax_len = 20\n\n# Perform greedy decoding\noutput_sequence = greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len)\n\n# Convert the output sequence to text\noutput_text = tokenizer_tgt.decode(output_sequence.tolist())\n# tokenizer_tgt.\n# Print the result\nprint(\"Input Text:\", source_text)\nprint(\"Output Text:\", output_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:17.631760Z","iopub.execute_input":"2024-06-26T21:39:17.632057Z","iopub.status.idle":"2024-06-26T21:39:17.865285Z","shell.execute_reply.started":"2024-06-26T21:39:17.632030Z","shell.execute_reply":"2024-06-26T21:39:17.864254Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tokenizer_en.json exists.\ntokenizer_it.json exists.\nInput Text: Hello, how are you?\nOutput Text: ladro fui Continuazione nascondendo soddisfatto iscoprire Provò studentato permetteranno esecutore albergarci Cedendo dondolo maravigliosa contrattino potente dispregio cagnacci\n","output_type":"stream"}]},{"cell_type":"code","source":"def bleu_loss(model, validation_ds, tokenizer_src, tokenizer_tgt, num_examples=2, max_len=20):\n    \"\"\"\n    Computes validation loss using a given model and dataset, and calculates BLEU score.\n\n    Args:\n        model (torch.nn.Module): The trained model to evaluate.\n        validation_ds (torch.utils.data.Dataset): Dataset for validation.\n        tokenizer_src (Tokenizer): Tokenizer for source language.\n        tokenizer_tgt (Tokenizer): Tokenizer for target language.\n        num_examples (int, optional): Number of examples to evaluate. Defaults to 2.\n        max_len (int, optional): Maximum length for decoding. Defaults to 50.\n        device (str, optional): Device for tensor computations ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        float: BLEU score for the predicted outputs compared to expected outputs.\n    \"\"\"\n    try:\n        model.eval()  # Set model to evaluation mode\n    except:\n        pass\n        \n    source_texts = []\n    expected = []\n    predicted = []\n\n    with torch.no_grad():\n        for count, batch in enumerate(validation_ds):\n            if count == num_examples:\n                break\n\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            # Generate predictions using greedy decoding\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len)\n\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = tokenizer_tgt.decode(model_out.tolist())\n\n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n\n    # Compute the BLEU metric\n    metric = BLEUScore()\n    bleu = metric(predicted, expected)\n\n#     print(source_texts, expected, predicted)\n    \n    return bleu\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:17.866661Z","iopub.execute_input":"2024-06-26T21:39:17.866956Z","iopub.status.idle":"2024-06-26T21:39:17.876405Z","shell.execute_reply.started":"2024-06-26T21:39:17.866929Z","shell.execute_reply":"2024-06-26T21:39:17.875517Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Example to test greedy_decode\nclass DummySeq2SeqModel:\n    def encode(self, source, source_mask):\n        # Fake encoding, in practice use your actual model\n        return torch.randn(1, source.size(1), 768)\n    \n    def decode(self, encoder_output, source_mask, decoder_input, decoder_mask):\n        # Fake decoding, in practice use your actual model\n        return torch.randn(1, decoder_input.size(1), 768)\n    \n    def project(self, decoder_output):\n        # Fake projection, in practice use your actual model\n        vocab_size = 30522  # Assume BERT tokenizer vocab size\n        return torch.randn(decoder_output.size(0), vocab_size)\n\nconfig = get_config()\n\ntrain_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n# Instantiate the dummy model\nmodel = DummySeq2SeqModel()\n\n# Define other parameters\nmax_len = 20\n\n# Perform greedy decoding\nloss = bleu_loss(model, val_dataloader, tokenizer_src, tokenizer_tgt, num_examples=2, max_len=5)\n\nprint(\"BLEUScore:\", float(loss))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:17.877576Z","iopub.execute_input":"2024-06-26T21:39:17.877872Z","iopub.status.idle":"2024-06-26T21:39:24.483312Z","shell.execute_reply.started":"2024-06-26T21:39:17.877846Z","shell.execute_reply":"2024-06-26T21:39:24.482386Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tokenizer_en.json exists.\ntokenizer_it.json exists.\nMax length for source senstences: 309\nMax length for target senstences: 309\nBLEUScore: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def custom_trainer(config):\n\n    # Make sure the weights folder exists\n    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    model = get_model(\n        config, \n        tokenizer_src.get_vocab_size(), \n        tokenizer_tgt.get_vocab_size()\n    ).to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    # If the user specified a model to preload before training, load it\n    initial_epoch = 0\n    global_step = 0\n    preload = config['preload']\n    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n\n    if model_filename:\n        print(f\"Preloading model: {model_filename}\")\n        state = torch.load(model_filename)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n    else:\n        print('No model to preload, starting from scratch')\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)    \n        \n    # Example tensors\n    # logits = torch.tensor([[2.0, 0.5, -0.5], [0.1, 1.0, -1.0], [0.2, 0.3, 0.8]], dtype=torch.float32)\n    # targets = torch.tensor([0, 1, 2], dtype=torch.int64)\n    # loss = loss_fn(logits, targets)\n    \n    for epoch in range(initial_epoch, config['num_epochs']):\n\n        torch.cuda.empty_cache()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n\n        for i, batch in tqdm(enumerate(batch_iterator)):\n                \n            model.train()\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n\n            # Run the tensors through the encoder, decoder and the projection layer\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n\n            # Compare the output with the label\n            label = batch['label'].to(device) # (B, seq_len)\n\n            # Compute the loss using a simple cross entropy\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            \n\n            # Backpropagate the loss\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            print('h')\n#             train_bleu_loss = bleu_loss(model, train_dataloader, tokenizer_src, tokenizer_tgt, num_examples=100, max_len=config['seq_len'])\n            val_bleu_loss = bleu_loss(model, val_dataloader, tokenizer_src, tokenizer_tgt, num_examples=100, max_len=config['seq_len'])\n            print('2')\n            # Log the loss\n            print(f'epoch_{epoch}__global_step_{global_step}__batch_no_{i}__train Cross Entropy loss_{loss.item()}__train BLEUloss_{float(val_bleu_loss)}__val BLEUloss_{float(val_bleu_loss)}')\n\n            global_step += 1\n\n        # Save the model at the end of every epoch\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:24.484657Z","iopub.execute_input":"2024-06-26T21:39:24.484964Z","iopub.status.idle":"2024-06-26T21:39:24.501120Z","shell.execute_reply.started":"2024-06-26T21:39:24.484936Z","shell.execute_reply":"2024-06-26T21:39:24.500214Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"config = get_config()\ncustom_trainer(config)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:24.502416Z","iopub.execute_input":"2024-06-26T21:39:24.503077Z","iopub.status.idle":"2024-06-26T21:39:35.747028Z","shell.execute_reply.started":"2024-06-26T21:39:24.503038Z","shell.execute_reply":"2024-06-26T21:39:35.745623Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"tokenizer_en.json exists.\ntokenizer_it.json exists.\nMax length for source senstences: 309\nMax length for target senstences: 309\nNo model to preload, starting from scratch\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Epoch 00:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20f4a8881c540e5b992050f577fab21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d188c1379b415bb4ce35c78c5eb4a5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcustom_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36mcustom_trainer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Run the tensors through the encoder, decoder and the projection layer\u001b[39;00m\n\u001b[1;32m     53\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(encoder_input, encoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Compare the output with the label\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, encoder_output, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(tgt)\n\u001b[1;32m     48\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pos(tgt)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/Decoder.py:46\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mForward pass through the decoder.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor after applying all decoder layers and layer normalization.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply each decoder layer to the input\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/DecoderBlock.py:56\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mForward pass through the decoder block.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor after applying self-attention, cross-attention, feed-forward network, and residual connections.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Apply the first residual connection with the self-attention block\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply the second residual connection with the cross-attention block\u001b[39;00m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_block(x, encoder_output, encoder_output, src_mask))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/ResidualConnection.py:39\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mApply residual connection to any sublayer.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor after applying the residual connection and dropout.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Norm and Add\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/DecoderBlock.py:56\u001b[0m, in \u001b[0;36mDecoderBlock.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mForward pass through the decoder block.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor after applying self-attention, cross-attention, feed-forward network, and residual connections.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Apply the first residual connection with the self-attention block\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply the second residual connection with the cross-attention block\u001b[39;00m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_block(x, encoder_output, encoder_output, src_mask))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/MultiHeadAttentionBlock.py:97\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Apply the attention mechanism\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttentionBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Reshape and transpose back to the original shape\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# (Batch, h, seq_len, d_k) -> (Batch, seq_len, h, d_k) -> (Batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n","File \u001b[0;32m/kaggle/working/transformer_pytorch/scripts/MultiHeadAttentionBlock.py:67\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.attention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     64\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39msoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (Batch, h, seq_len, seq_len)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# (batch. h. seq_len, seq_len) -> (batch, h, seq_len, d_k)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# also resturn attention scores used for visualization\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (attention_scores \u001b[38;5;241m@\u001b[39m value), attention_scores\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 0\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T21:39:35.747897Z","iopub.status.idle":"2024-06-26T21:39:35.748230Z","shell.execute_reply.started":"2024-06-26T21:39:35.748071Z","shell.execute_reply":"2024-06-26T21:39:35.748085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}